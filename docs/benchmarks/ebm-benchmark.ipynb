{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc27b1-7903-4d36-9198-190923b85d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use exact versions of these in order to preserve RANK ordering better\n",
    "requirements = \"numpy==1.26.4 pandas==2.2.2 scikit-learn==1.5.1 xgboost==2.1.0 lightgbm==4.5.0 catboost==1.2.5 aplr==10.6.1\"\n",
    "!pip install -U --quiet {requirements}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8489becf-d522-42a0-af6e-ec99c12d6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install interpret if not already installed\n",
    "try:\n",
    "    import interpret\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -U --quiet interpret-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5674068-d971-49df-b8a1-60bf91441def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install powerlift if not already installed\n",
    "try:\n",
    "    import powerlift\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -U --quiet powerlift[datasets,postgres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7cfe45-d0f1-4951-953e-cafefee1ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_filter(task):\n",
    "    min_samples = 1\n",
    "    max_samples = 1000000000000\n",
    "    min_features = 1\n",
    "    max_features = 1000000000000\n",
    "    if task.scalar_measure(\"n_rows\") < min_samples:\n",
    "        return []\n",
    "    if max_samples < task.scalar_measure(\"n_rows\"):\n",
    "        return []\n",
    "    if task.scalar_measure(\"n_cols\") < min_features:\n",
    "        return []\n",
    "    if max_features < task.scalar_measure(\"n_cols\"):\n",
    "        return []\n",
    "\n",
    "    \n",
    "    if task.origin == \"openml_automl_regression\":\n",
    "        pass  # include in benchmark\n",
    "    elif task.origin == \"openml_automl_classification\":\n",
    "        return []\n",
    "    elif task.origin == \"openml_cc18\":\n",
    "        pass  # include in benchmark\n",
    "    elif task.origin == \"pmlb\":\n",
    "        if task.problem == \"binary\":\n",
    "            return []\n",
    "        elif task.problem == \"multiclass\":\n",
    "            return []\n",
    "        elif task.problem == \"regression\":\n",
    "            return []\n",
    "        else:\n",
    "            raise Exception(f\"Unrecognized problem {task.problem}\")\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized origin {task.origin}\")\n",
    "\n",
    "    \n",
    "    exclude_set = set()\n",
    "#    exclude_set = set(['isolet', 'Devnagari-Script', 'CIFAR_10', 'Airlines_DepDelay_10M'])\n",
    "#    exclude_set = set([\n",
    "#        'Fashion-MNIST', 'mfeat-pixel', 'Bioresponse',\n",
    "#        'mfeat-factors', 'isolet', 'cnae-9', \"Internet-Advertisements\",\n",
    "#        'har', 'Devnagari-Script', 'mnist_784', 'CIFAR_10',\n",
    "#        'Airlines_DepDelay_10M',\n",
    "#    ])\n",
    "    if task.name in exclude_set:\n",
    "        return []\n",
    "\n",
    "\n",
    "    # exclude duplicates of a dataset if they appear twice\n",
    "    global global_duplicates\n",
    "    try:\n",
    "        duplicates = global_duplicates\n",
    "    except NameError:\n",
    "        duplicates = set()\n",
    "        global_duplicates = duplicates\n",
    "    key = (task.name, task.scalar_measure(\"n_rows\"), task.scalar_measure(\"n_cols\"))\n",
    "    if key in duplicates:\n",
    "        print(f\"Excluding duplicate: {key}\")\n",
    "        return []\n",
    "    else:\n",
    "        duplicates.add(key)\n",
    "\n",
    "\n",
    "    return [\n",
    "        \"ebm\",\n",
    "        \"xgb\",\n",
    "        \"lgbm\",\n",
    "        \"catboost\",\n",
    "        \"rf_xgb\",\n",
    "        \"rf_sk\",\n",
    "        \"linear_sv\",\n",
    "        # \"aplr\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b43ca15-4a20-4622-a877-0a5af322b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_runner(trial):\n",
    "    seed=42 + int(trial.replicate_num)\n",
    "\n",
    "    from interpret.glassbox import ExplainableBoostingClassifier, ExplainableBoostingRegressor\n",
    "    from xgboost import XGBClassifier, XGBRegressor, XGBRFClassifier, XGBRFRegressor\n",
    "    from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "    from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "    from sklearn.svm import LinearSVC, LinearSVR\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    from aplr import APLRClassifier, APLRRegressor\n",
    "    from sklearn.metrics import roc_auc_score, root_mean_squared_error, log_loss\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import numpy as np\n",
    "    from time import time\n",
    "    import warnings\n",
    "\n",
    "    X, y, meta = trial.task.data([\"X\", \"y\", \"meta\"])\n",
    "\n",
    "    for col in X.columns:\n",
    "        # catboost doesn't like missing categoricals, so make them a category\n",
    "        col_data = X[col]\n",
    "        if str(col_data.dtype) == \"category\" and col_data.isnull().any():\n",
    "            X[col] = col_data.cat.add_categories('nan').fillna('nan')\n",
    "    \n",
    "    cat_bools = meta[\"categorical_mask\"]\n",
    "    cat_cols = [i for i, val in enumerate(cat_bools) if val]\n",
    "    num_cols = [i for i, val in enumerate(cat_bools) if not val]\n",
    "    \n",
    "    stratification = None\n",
    "    if trial.task.problem in [\"binary\", \"multiclass\"]:\n",
    "        # stratification = y\n",
    "        pass  # Re-enable stratification if dataset fails from absent class in train/test sets (PMLB)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=stratification, random_state=seed)\n",
    "\n",
    "    # Build optional preprocessor for use by methods below\n",
    "    # missing categoricals already handled above by making new \"nan\" category\n",
    "    cat_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True, dtype=np.int16)\n",
    "    num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "    transformers = [(\"cat\", cat_encoder, cat_cols), (\"num\", num_imputer, num_cols)]\n",
    "    ct = ColumnTransformer(transformers=transformers, sparse_threshold=1.0)  # densify or sparsify\n",
    "\n",
    "\n",
    "    ebm_params = {}\n",
    "    xgb_params = {}\n",
    "    lgbm_params = {}\n",
    "    catboost_params = {}\n",
    "    rf_xgb_params = {}\n",
    "    rf_sk_params = {}\n",
    "    linear_sv_params = {}\n",
    "    aplr_params = {}\n",
    "\n",
    "    ebm_params[\"feature_types\"] = [\"nominal\" if cat else \"continuous\" for cat in cat_bools]\n",
    "    ebm_params[\"n_jobs\"] = -1\n",
    "    xgb_params[\"enable_categorical\"] = True\n",
    "    xgb_params[\"feature_types\"] = [\"c\" if cat else \"q\" for cat in cat_bools]\n",
    "    lgbm_params[\"verbosity\"] = -1\n",
    "    catboost_params[\"verbose\"] = False\n",
    "    rf_xgb_params[\"enable_categorical\"] = True\n",
    "    rf_xgb_params[\"feature_types\"] = [\"c\" if cat else \"q\" for cat in cat_bools]\n",
    "    rf_sk_params[\"n_jobs\"] = -1\n",
    "    aplr_params[\"m\"] = 3000\n",
    "\n",
    "    if 1000 < trial.task.scalar_measure(\"n_cols\"):\n",
    "        # TODO: EBMs can crash for now with too many interactions, so limit it until we have better fix\n",
    "        ebm_params[\"interactions\"] = 0\n",
    "\n",
    "    # DEBUG params to make the algorithms super fast\n",
    "    #if 10000 < len(y_train):\n",
    "    #    debug_stratify = y_train if trial.task.problem in [\"binary\", \"multiclass\"] else None\n",
    "    #    X_train, _, y_train, _ = train_test_split(X_train, y_train, test_size=len(y_train) - 5000, stratify=debug_stratify, random_state=seed)\n",
    "    #ebm_params[\"max_rounds\"] = 1\n",
    "    #ebm_params[\"interactions\"] = 0\n",
    "    #xgb_params[\"n_estimators\"] = 1\n",
    "    #lgbm_params[\"n_estimators\"] = 1\n",
    "    #catboost_params[\"n_estimators\"] = 1\n",
    "    #rf_xgb_params[\"n_estimators\"] = 1\n",
    "    #rf_sk_params[\"n_estimators\"] = 1\n",
    "    #aplr_params[\"m\"] = 1\n",
    "    \n",
    "    # Specify method\n",
    "    if trial.task.problem in [\"binary\", \"multiclass\"]:\n",
    "        fit_params = {\"X\":X_train, \"y\":y_train}\n",
    "        if trial.method.name == \"ebm\":\n",
    "            est = ExplainableBoostingClassifier(**ebm_params)\n",
    "        elif trial.method.name == \"xgb\":\n",
    "            est = XGBClassifier(**xgb_params)\n",
    "            fit_params[\"verbose\"] = False\n",
    "        elif trial.method.name == \"lgbm\":\n",
    "            est = LGBMClassifier(**lgbm_params)\n",
    "            fit_params[\"categorical_feature\"] = cat_cols\n",
    "        elif trial.method.name == \"catboost\":\n",
    "            est = CatBoostClassifier(**catboost_params)\n",
    "            fit_params[\"cat_features\"] = cat_cols\n",
    "        elif trial.method.name == \"rf_xgb\":\n",
    "            est = XGBRFClassifier(**rf_xgb_params)\n",
    "            fit_params[\"verbose\"] = False\n",
    "        elif trial.method.name == \"rf_sk\":\n",
    "            est = Pipeline([(\"ct\", ct), (\"est\", RandomForestClassifier(**rf_sk_params))])\n",
    "        elif trial.method.name == \"linear_sv\":\n",
    "            if trial.task.problem == \"multiclass\":\n",
    "                est = Pipeline([(\"ct\", ct), (\"est\", CalibratedClassifierCV(OneVsRestClassifier(LinearSVC(**linear_sv_params), n_jobs=-1)))])\n",
    "            else:\n",
    "                est = Pipeline([(\"ct\", ct), (\"est\", CalibratedClassifierCV(LinearSVC(**linear_sv_params)))])\n",
    "        elif trial.method.name == \"aplr\":\n",
    "            est = Pipeline(\n",
    "                [\n",
    "                    (\"ct\", ct),\n",
    "                    (\n",
    "                        \"est\",\n",
    "                        APLRClassifier(**aplr_params),\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            y_train = y_train.astype(str).to_numpy()\n",
    "            y_test = y_test.astype(str).to_numpy()\n",
    "            fit_params = {\"X\":X_train, \"y\":y_train}\n",
    "        else:\n",
    "            raise Exception(f\"Unrecognized method name {trial.method.name}\")\n",
    "\n",
    "        predict_fn = est.predict_proba\n",
    "    elif trial.task.problem == \"regression\":\n",
    "        fit_params = {\"X\":X_train, \"y\":y_train}\n",
    "        if trial.method.name == \"ebm\":\n",
    "            est = ExplainableBoostingRegressor(**ebm_params)\n",
    "        elif trial.method.name == \"xgb\":\n",
    "            est = XGBRegressor(**xgb_params)\n",
    "            fit_params[\"verbose\"] = False\n",
    "        elif trial.method.name == \"lgbm\":\n",
    "            est = LGBMRegressor(**lgbm_params)\n",
    "            fit_params[\"categorical_feature\"] = cat_cols\n",
    "        elif trial.method.name == \"catboost\":\n",
    "            est = CatBoostRegressor(**catboost_params)\n",
    "            fit_params[\"cat_features\"] = cat_cols\n",
    "        elif trial.method.name == \"rf_xgb\":\n",
    "            est = XGBRFRegressor(**rf_xgb_params)\n",
    "            fit_params[\"verbose\"] = False\n",
    "        elif trial.method.name == \"rf_sk\":\n",
    "            est = Pipeline([(\"ct\", ct), (\"est\", RandomForestRegressor(**rf_sk_params))])\n",
    "        elif trial.method.name == \"linear_sv\":\n",
    "            est = Pipeline([(\"ct\", ct), (\"est\", LinearSVR(**linear_sv_params))])\n",
    "        elif trial.method.name == \"aplr\":\n",
    "            est = Pipeline(\n",
    "                [\n",
    "                    (\"ct\", ct),\n",
    "                    (\n",
    "                        \"est\",\n",
    "                        APLRRegressor(**aplr_params),\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            y_train = y_train.astype(str).to_numpy()\n",
    "            y_test = y_test.astype(str).to_numpy()\n",
    "            fit_params = {\"X\":X_train, \"y\":y_train}\n",
    "        else:\n",
    "            raise Exception(f\"Unrecognized method name {trial.method.name}\")\n",
    "\n",
    "        predict_fn = est.predict\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized problem {trial.task.problem}\")\n",
    "\n",
    "    global global_counter\n",
    "    try:\n",
    "        global_counter += 1\n",
    "    except NameError:\n",
    "        global_counter = 0\n",
    "    \n",
    "    # Train\n",
    "    print(f\"FIT: {global_counter}, {trial.task.origin}, {trial.task.name}, {trial.method.name}, classes:{int(trial.task.scalar_measure('n_classes'))}, features:{int(trial.task.scalar_measure('n_cols'))}, samples:{int(trial.task.scalar_measure('n_rows'))}\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        start_time = time()\n",
    "        est.fit(**fit_params)\n",
    "        elapsed_time = time() - start_time\n",
    "    trial.log(\"fit_time\", elapsed_time)\n",
    "    \n",
    "    # Predict\n",
    "    start_time = time()\n",
    "    predictions = predict_fn(X_test)\n",
    "    elapsed_time = time() - start_time\n",
    "    trial.log(\"predict_time\", elapsed_time)\n",
    "\n",
    "    if trial.task.problem == \"binary\":\n",
    "        predictions = predictions[:,1]\n",
    "\n",
    "        eval_score = roc_auc_score(y_test, predictions)\n",
    "        trial.log(\"auc\", eval_score)\n",
    "\n",
    "        eval_score2 = log_loss(y_test, predictions)\n",
    "        trial.log(\"log_loss\", eval_score2)\n",
    "    elif trial.task.problem == \"multiclass\":\n",
    "        eval_score = roc_auc_score(y_test, predictions, average=\"weighted\", multi_class=\"ovo\")\n",
    "        trial.log(\"multi_auc\", eval_score)\n",
    "\n",
    "        eval_score2 = log_loss(y_test, predictions)\n",
    "        trial.log(\"cross_entropy\", eval_score2)\n",
    "    elif trial.task.problem == \"regression\":\n",
    "        # Use NRMSE-IQR (normalized root mean square error by the interquartile range)\n",
    "        # so that datasets with large predicted values do not dominate the benchmark\n",
    "        # and the range is not sensitive to outliers. The rank is identical to RMSE.\n",
    "        # https://en.wikipedia.org/wiki/Root_mean_square_deviation\n",
    "\n",
    "        # Get quartile_range from the full dataset for consistency across seeds.\n",
    "        q75, q25 = np.percentile(y, [75, 25])\n",
    "        interquartile_range = q75 - q25\n",
    "\n",
    "        eval_score = root_mean_squared_error(y_test, predictions) / interquartile_range\n",
    "        trial.log(\"nrmse\", eval_score)\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized problem {trial.task.problem}\")\n",
    "\n",
    "    print(eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c226e77-753a-4e1c-bb6b-d80156845785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "experiment_name = datetime.datetime.now().strftime(\"%Y_%m_%d_%H%M__\") + \"myexperiment\"\n",
    "# experiment_name = \"yyyy_mm_dd_hhmm__myexperiment\"\n",
    "\n",
    "print(\"Experiment name: \" + experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ee509-945c-4376-aab9-72387298bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_recreate=False\n",
    "exist_ok=True\n",
    "is_local=True\n",
    "n_replicates=10\n",
    "\n",
    "import os\n",
    "if is_local:\n",
    "    conn_str = f\"sqlite:///{os.getcwd()}/powerlift.db\"\n",
    "else:\n",
    "    from azure.identity import AzureCliCredential\n",
    "    credential = AzureCliCredential()\n",
    "    \n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    TIMEOUT_SEC = 60 * 60 * 24 * 180  # 180 days\n",
    "    wheel_filepaths = [\"interpret_core-0.6.3-py3-none-any.whl\", \"powerlift-0.1.11-py3-none-any.whl\"]\n",
    "    n_containers=198\n",
    "    conn_str = os.getenv(\"DOCKER_DB_URL\")\n",
    "    azure_tenant_id = os.getenv(\"AZURE_TENANT_ID\")\n",
    "    azure_client_id = os.getenv(\"AZURE_CLIENT_ID\")\n",
    "    azure_client_secret = os.getenv(\"AZURE_CLIENT_SECRET\")\n",
    "    subscription_id = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
    "    resource_group = os.getenv(\"AZURE_RESOURCE_GROUP\")\n",
    "\n",
    "from powerlift.bench import retrieve_openml_automl_regression, retrieve_openml_automl_classification, retrieve_openml_cc18, retrieve_catboost_50k, retrieve_pmlb\n",
    "from powerlift.bench import Benchmark, Store, populate_with_datasets\n",
    "from powerlift.executors import LocalMachine, AzureContainerInstance\n",
    "from itertools import chain\n",
    "\n",
    "# Initialize database (if needed).\n",
    "store = Store(conn_str, force_recreate=force_recreate, print_exceptions=True)\n",
    "\n",
    "benchmark = Benchmark(store, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7ed60-fbd0-46b1-8bd9-b140ec31a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir=\"~/.powerlift\"\n",
    "data_retrieval = chain(\n",
    "    retrieve_openml_automl_regression(cache_dir=cache_dir),\n",
    "    # retrieve_openml_automl_classification(cache_dir=cache_dir),\n",
    "    retrieve_openml_cc18(cache_dir=cache_dir),\n",
    "    # retrieve_catboost_50k(cache_dir=cache_dir),\n",
    "    # retrieve_pmlb(cache_dir=cache_dir),\n",
    ")\n",
    "\n",
    "# This downloads datasets once and feeds into the database.\n",
    "populate_with_datasets(store, data_retrieval, exist_ok=exist_ok)\n",
    "\n",
    "if is_local:\n",
    "    benchmark.run(trial_runner, trial_filter, n_replicates=n_replicates, executor=LocalMachine(store, debug_mode=True))\n",
    "else:\n",
    "    executor = AzureContainerInstance(\n",
    "        store, azure_tenant_id, azure_client_id, azure_client_secret, subscription_id, resource_group, credential,\n",
    "        image=\"mcr.microsoft.com/devcontainers/python:latest\",\n",
    "        pip_install= requirements + \" psycopg2-binary\",\n",
    "        wheel_filepaths=wheel_filepaths,\n",
    "        n_running_containers=n_containers, num_cores=4, mem_size_gb=16, delete_group_container_on_complete=True\n",
    "    )\n",
    "    benchmark.run(trial_runner, trial_filter, timeout=TIMEOUT_SEC, n_replicates=n_replicates, executor=executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5745059-f747-4c02-8720-0b2d49aa4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark.wait_until_complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = benchmark.results()\n",
    "results_df['meta'] = results_df['meta'].apply(lambda x: str(x))\n",
    "results_df = results_df.sort_values(by=['task', 'method', 'meta', 'replicate_num', 'name'])\n",
    "results_df.to_csv(f\"{experiment_name}.csv\", index=None)\n",
    "\n",
    "status_df = benchmark.status()\n",
    "for errmsg in status_df[\"errmsg\"]:\n",
    "    if errmsg is not None:\n",
    "        print(\"ERROR: \" + str(errmsg))\n",
    "print(status_df['status'].value_counts().to_string(index=True, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2716b066-7d8b-4163-8d5d-a1b7f0274dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reload if analyzing later\n",
    "results_df = pd.read_csv(f\"{experiment_name}.csv\")\n",
    "\n",
    "averages = results_df.groupby(['method', 'name'])['num_val'].mean().unstack().reset_index()\n",
    "\n",
    "metric_ranks = results_df.pivot_table('num_val', ['task', 'name'], ['method', 'replicate_num'])\n",
    "metric_ranks = metric_ranks.rank(axis=1, ascending=True, method='min')\n",
    "metric_ranks = metric_ranks.stack(level='replicate_num', future_stack=True)\n",
    "metric_ranks = metric_ranks.groupby('name').mean().transpose()\n",
    "metric_ranks.columns = [f\"{col}_RANK\" for col in metric_ranks.columns]\n",
    "metric_ranks = metric_ranks.reset_index()\n",
    "\n",
    "overall_rank = results_df[results_df['name'].isin(['log_loss', 'cross_entropy', 'nrmse'])]\n",
    "overall_rank = overall_rank.pivot_table('num_val', 'task', ['method', 'replicate_num'])\n",
    "overall_rank = overall_rank.rank(axis=1, ascending=True, method='min')\n",
    "overall_rank = overall_rank.stack(level='replicate_num', future_stack=True)\n",
    "overall_rank = overall_rank.mean()\n",
    "overall_rank = overall_rank.to_frame(name='RANK').reset_index()\n",
    "\n",
    "desired_columns = ['method', 'RANK', 'auc', 'multi_auc', 'nrmse', 'log_loss_RANK', 'cross_entropy_RANK', 'nrmse_RANK', 'fit_time', 'predict_time']\n",
    "combined_df = averages.merge(metric_ranks, on='method').merge(overall_rank, on='method')\n",
    "combined_df = combined_df.sort_values(by='RANK')\n",
    "combined_df = combined_df.reindex(columns=desired_columns)\n",
    "\n",
    "print(combined_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2df600-d3e0-430e-8dda-836c09825987",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_columns = ['method', 'RANK', 'auc', 'multi_auc', 'nrmse', 'log_loss', 'cross_entropy', 'fit_time', 'predict_time']\n",
    "row_order = combined_df[\"method\"]\n",
    "\n",
    "counts = results_df.groupby(['method', 'name']).size().unstack()\n",
    "counts = counts.reindex(row_order, axis=0).reset_index()\n",
    "counts['RANK'] = 0\n",
    "if 'log_loss' in counts.columns:\n",
    "    counts['RANK'] += counts['log_loss']\n",
    "if 'cross_entropy' in counts.columns:\n",
    "    counts['RANK'] += counts['cross_entropy']\n",
    "if 'nrmse' in counts.columns:\n",
    "    counts['RANK'] += counts['nrmse']\n",
    "counts = counts.reindex(columns=desired_columns)\n",
    "print(counts.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a87a2-3dd7-4cef-905f-ed00b3a1065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = results_df[results_df['name'].isin(['log_loss', 'cross_entropy', 'nrmse'])]\n",
    "grouped = filtered_df.groupby(['task', 'method', 'name'])['num_val'].agg(['mean']).reset_index()\n",
    "pivot_table = grouped.pivot_table(index=['task', 'name'], columns=['method'], values=['mean'])\n",
    "pivot_table.columns = pivot_table.columns.droplevel(0)\n",
    "pivot_table = pivot_table.reindex(row_order, axis=1)\n",
    "pivot_table['ratio'] = pivot_table['ebm'] / pivot_table['xgb']\n",
    "pivot_table = pivot_table.sort_values(by='ratio')\n",
    "task_order = pivot_table.index\n",
    "pivot_table = pivot_table.reset_index()\n",
    "print(pivot_table.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077607b5-06d5-4673-bb93-c9cd527c73fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = results_df[results_df['name'].isin(['log_loss', 'cross_entropy', 'nrmse'])]\n",
    "grouped = filtered_df.groupby(['task', 'method', 'name'])['num_val'].agg(['std']).reset_index()\n",
    "pivot_table = grouped.pivot_table(index=['task', 'name'], columns=['method'], values=['std'])\n",
    "pivot_table.columns = pivot_table.columns.droplevel(0)\n",
    "pivot_table = pivot_table.reindex(row_order, axis=1)\n",
    "pivot_table = pivot_table.reindex(task_order, axis=0).reset_index()\n",
    "print(pivot_table.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321648e-3fc6-486f-af3c-c07eca46ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = results_df[results_df['name'].isin(['log_loss', 'cross_entropy', 'nrmse'])]\n",
    "grouped = filtered_df.groupby(['task', 'method', 'name'])['num_val'].agg(['count']).reset_index()\n",
    "pivot_table = grouped.pivot_table(index=['task', 'name'], columns=['method'], values=['count'])\n",
    "pivot_table = pivot_table.fillna(0).astype(int)\n",
    "pivot_table.columns = pivot_table.columns.droplevel(0)\n",
    "pivot_table = pivot_table.reindex(row_order, axis=1)\n",
    "pivot_table = pivot_table.reindex(task_order, axis=0).reset_index()\n",
    "print(pivot_table.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0e10e-6825-4764-84f9-934d1ed4e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_times = results_df[results_df['name'] == 'fit_time']\n",
    "fit_times = fit_times.pivot_table('num_val', 'task', 'method')\n",
    "fit_times = fit_times.dropna()\n",
    "fit_times[\"ratios\"] = fit_times['ebm'] / fit_times['xgb']\n",
    "import numpy as np\n",
    "fit_times_deciles = np.percentile(fit_times[\"ratios\"], [90, 80, 70, 60, 50, 40, 30, 20, 10])\n",
    "fit_times_deciles = [f\"{decile:.2f}  \" for decile in fit_times_deciles]\n",
    "max_ratio= fit_times[\"ratios\"].max()\n",
    "min_ratio= fit_times[\"ratios\"].min()\n",
    "print(\"fit time ratio deciles:\")\n",
    "print(*fit_times_deciles)\n",
    "print(f\"max: {max_ratio:.2f}\")\n",
    "print(f\"min: {min_ratio:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
