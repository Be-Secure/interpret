{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc27b1-7903-4d36-9198-190923b85d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install interpret if not already installed\n",
    "try:\n",
    "    import interpret\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -U --quiet scikit-learn xgboost interpret-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5674068-d971-49df-b8a1-60bf91441def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install powerlift if not already installed\n",
    "try:\n",
    "    import powerlift\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -U --quiet powerlift[datasets,postgres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7cfe45-d0f1-4951-953e-cafefee1ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_filter(task):\n",
    "    min_samples = 1\n",
    "    max_samples = 1000000000000\n",
    "    \n",
    "    if task.scalar_measure(\"n_rows\") < min_samples:\n",
    "        return []\n",
    "\n",
    "    if max_samples < task.scalar_measure(\"n_rows\"):\n",
    "        return []\n",
    "\n",
    "    if task.origin == \"openml\":\n",
    "        exclude_set = set()\n",
    "#        exclude_set = set(['isolet', 'Devnagari-Script', 'CIFAR_10'])\n",
    "#        exclude_set = set([\n",
    "#            'Fashion-MNIST', 'mfeat-pixel', 'Bioresponse',\n",
    "#            'mfeat-factors', 'isolet', 'cnae-9', \"Internet-Advertisements\",\n",
    "#            'har', 'Devnagari-Script', 'mnist_784', 'CIFAR_10',\n",
    "#        ])\n",
    "        if task.name in exclude_set:\n",
    "            return []\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized task origin {task.origin}\")\n",
    "\n",
    "    return [\n",
    "        \"xgboost-base\",\n",
    "        \"ebm-base\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b43ca15-4a20-4622-a877-0a5af322b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_runner(trial):\n",
    "    seed=42\n",
    "    extra_params = {}\n",
    "    # extra_params = {\"interactions\":0, \"max_rounds\":5}\n",
    "    \n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "    from interpret.glassbox import ExplainableBoostingClassifier, ExplainableBoostingRegressor\n",
    "    from sklearn.metrics import roc_auc_score, root_mean_squared_error\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from time import time\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "\n",
    "    X, y, meta = trial.task.data([\"X\", \"y\", \"meta\"])\n",
    "\n",
    "    # TODO: move this into powerlift\n",
    "    for col_name in X.columns:\n",
    "        col = X[col_name]\n",
    "        if col.dtype.name == 'object':\n",
    "            X[col_name] = col.astype(pd.CategoricalDtype(ordered=False))\n",
    "        elif col.dtype.name == 'category' and col.cat.ordered:\n",
    "            X[col_name] = col.cat.as_unordered()\n",
    "    import numpy as np\n",
    "    _, y = np.unique(y, return_inverse=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "\n",
    "    # Specify method\n",
    "    if trial.task.problem in [\"binary\", \"multiclass\"]:\n",
    "        if trial.method.name == \"xgboost-base\":\n",
    "            est = XGBClassifier(enable_categorical=True)\n",
    "        elif trial.method.name == \"ebm-base\":\n",
    "            est = ExplainableBoostingClassifier(**extra_params)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Method unavailable for {trial.method.name}\")\n",
    "\n",
    "        if trial.task.problem == \"binary\":\n",
    "            predict_fn = lambda x: est.predict_proba(x)[:,1]\n",
    "            eval_name = \"auc\"\n",
    "            eval_fn = roc_auc_score\n",
    "            eval_params = {}\n",
    "        elif trial.task.problem == \"multiclass\":\n",
    "            predict_fn = lambda x: est.predict_proba(x)\n",
    "            eval_name = \"multi_auc\"\n",
    "            eval_fn = roc_auc_score\n",
    "            eval_params = {\"average\": \"weighted\", \"multi_class\": \"ovr\"}\n",
    "    elif trial.task.problem == \"regression\":\n",
    "        if trial.method.name == \"xgboost-base\":\n",
    "            est = XGBRegressor(enable_categorical=True)\n",
    "        elif trial.method.name == \"ebm-base\":\n",
    "            est = ExplainableBoostingRegressor(**extra_params)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Method unavailable for {trial.method.name}\")\n",
    "            \n",
    "        q75, q25 = np.percentile(y_train, [75, 25])\n",
    "        interquartile_range = q75 - q25\n",
    "    \n",
    "        predict_fn = lambda x: est.predict(x)\n",
    "        eval_name = \"rmsdiqr\"\n",
    "        eval_fn = lambda y_true, y_pred: -np.sqrt(np.mean((y_true - y_pred) ** 2)) / interquartile_range\n",
    "        eval_params = {}\n",
    "\n",
    "    global global_counter\n",
    "    try:\n",
    "        global_counter += 1\n",
    "    except NameError:\n",
    "        global_counter = 1\n",
    "    \n",
    "    # Train\n",
    "    start_time = time()\n",
    "    print(f\"FIT: {global_counter}, {trial.task.origin}, {trial.task.name}, {trial.method.name}, \", end=\"\")\n",
    "    with warnings.catch_warnings():  \n",
    "        warnings.filterwarnings(\"ignore\") \n",
    "        est.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    trial.log(\"fit_time\", elapsed_time)\n",
    "    \n",
    "    # Predict\n",
    "    start_time = time()\n",
    "    predictions = predict_fn(X_test)\n",
    "    end_time = time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    trial.log(\"predict_time\", elapsed_time)\n",
    "\n",
    "    # Score\n",
    "    eval_score = eval_fn(y_test, predictions, **eval_params)\n",
    "    trial.log(eval_name, eval_score)\n",
    "\n",
    "    print(eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c226e77-753a-4e1c-bb6b-d80156845785",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_recreate=False\n",
    "exist_ok=True\n",
    "\n",
    "import uuid\n",
    "experiment_name = \"myexperiment\" + \"__\" + str(uuid.uuid4())\n",
    "print(\"Experiment name: \" + str(experiment_name))\n",
    "\n",
    "from powerlift.bench import retrieve_openml, retrieve_pmlb, retrieve_catboost_50k\n",
    "from powerlift.bench import Benchmark, Store, populate_with_datasets\n",
    "from powerlift.executors import LocalMachine\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "# Initialize database (if needed).\n",
    "store = Store(f\"sqlite:///{os.getcwd()}/powerlift.db\", force_recreate=force_recreate)\n",
    "\n",
    "cache_dir=\"~/.powerlift\"\n",
    "data_retrieval = chain(\n",
    "    # retrieve_catboost_50k(cache_dir=cache_dir),\n",
    "    # retrieve_pmlb(cache_dir=cache_dir),\n",
    "    retrieve_openml(cache_dir=cache_dir),\n",
    ")\n",
    "\n",
    "# This downloads datasets once and feeds into the database.\n",
    "populate_with_datasets(store, data_retrieval, exist_ok=exist_ok)\n",
    "\n",
    "# Run experiment\n",
    "benchmark = Benchmark(store, name=experiment_name)\n",
    "benchmark.run(trial_runner, trial_filter, executor=LocalMachine(store, debug_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79655404-4f7b-474a-aa6c-d74cb831980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.wait_until_complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70b5dc-d0ef-4b4a-93be-1fd8a60e2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-establish connection\n",
    "#benchmark = Benchmark(conn_str, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b43b3-8d9f-4cfd-81bf-4d4200deb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df = benchmark.status()\n",
    "error_idx = status_df[\"errmsg\"].first_valid_index()\n",
    "if error_idx is not None:\n",
    "    print(\"ERROR: \" + str(status_df[\"errmsg\"][error_idx]))\n",
    "print(status_df['status'].value_counts().to_string(index=True, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e071c6-655b-4800-852c-1b05a982156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = benchmark.results()\n",
    "results_df.to_csv('results.csv', index=None)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b6e6b-84b2-49ec-a9da-1b26f4b70403",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.groupby(['method', 'name'])['num_val'].agg(['mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e28f4-7b6a-4439-8ad4-de9aafcaf0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = results_df[results_df['name'].isin(['auc', 'multi_auc', 'rmsdiqr'])]\n",
    "pivot_df = filtered_df.pivot_table(index=['task', 'name'], columns='method', values='num_val')\n",
    "ranks_df = pivot_df.rank(axis=1, ascending=False, method='min')\n",
    "average_ranks = ranks_df.groupby(level='name').mean()\n",
    "print(average_ranks)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
