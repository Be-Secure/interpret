{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc27b1-7903-4d36-9198-190923b85d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install interpret if not already installed\n",
    "try:\n",
    "    import interpret\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -U --quiet scikit-learn xgboost interpret-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5674068-d971-49df-b8a1-60bf91441def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install powerlift if not already installed\n",
    "try:\n",
    "    import powerlift\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -U --quiet powerlift[datasets,postgres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7cfe45-d0f1-4951-953e-cafefee1ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_filter(task):\n",
    "    min_samples = 1\n",
    "    max_samples = 1000000000000\n",
    "    \n",
    "    if task.scalar_measure(\"n_rows\") < min_samples:\n",
    "        return []\n",
    "\n",
    "    if max_samples < task.scalar_measure(\"n_rows\"):\n",
    "        return []\n",
    "\n",
    "    if task.origin == \"openml\":\n",
    "        exclude_set = set()\n",
    "#        exclude_set = set(['isolet', 'Devnagari-Script', 'CIFAR_10'])\n",
    "#        exclude_set = set([\n",
    "#            'Fashion-MNIST', 'mfeat-pixel', 'Bioresponse',\n",
    "#            'mfeat-factors', 'isolet', 'cnae-9', \"Internet-Advertisements\",\n",
    "#            'har', 'Devnagari-Script', 'mnist_784', 'CIFAR_10',\n",
    "#        ])\n",
    "        if task.name in exclude_set:\n",
    "            return []\n",
    "    elif task.origin == \"pmlb\":\n",
    "        exclude_set = set()\n",
    "        if task.name in exclude_set:\n",
    "            return []\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized task origin {task.origin}\")\n",
    "\n",
    "    return [\n",
    "        \"xgboost-base\",\n",
    "        \"ebm-base\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b43ca15-4a20-4622-a877-0a5af322b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_runner(trial):\n",
    "    seed=42\n",
    "    extra_params = {}\n",
    "    # extra_params = {\"interactions\":0, \"max_rounds\":5}\n",
    "    \n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "    from interpret.glassbox import ExplainableBoostingClassifier, ExplainableBoostingRegressor\n",
    "    from sklearn.metrics import roc_auc_score, root_mean_squared_error, log_loss\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from time import time\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "\n",
    "    X, y, meta = trial.task.data([\"X\", \"y\", \"meta\"])\n",
    "\n",
    "    # TODO: move this into powerlift\n",
    "    for col_name in X.columns:\n",
    "        col = X[col_name]\n",
    "        if col.dtype.name == 'object':\n",
    "            X[col_name] = col.astype(pd.CategoricalDtype(ordered=False))\n",
    "        elif col.dtype.name == 'category' and col.cat.ordered:\n",
    "            X[col_name] = col.cat.as_unordered()\n",
    "    import numpy as np\n",
    "    _, y = np.unique(y, return_inverse=True)\n",
    "\n",
    "    stratification = None\n",
    "    if trial.task.problem in [\"binary\", \"multiclass\"]:\n",
    "        # use stratefied, otherwise eval can fail if one of the classes is not in the training set\n",
    "        stratification = y\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=stratification, random_state=seed)\n",
    "\n",
    "    # Specify method\n",
    "    eval_fn2 = None\n",
    "    if trial.task.problem in [\"binary\", \"multiclass\"]:\n",
    "        if trial.method.name == \"xgboost-base\":\n",
    "            est = XGBClassifier(enable_categorical=True)\n",
    "        elif trial.method.name == \"ebm-base\":\n",
    "            est = ExplainableBoostingClassifier(**extra_params)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Method unavailable for {trial.method.name}\")\n",
    "\n",
    "        if trial.task.problem == \"binary\":\n",
    "            predict_fn = lambda x: est.predict_proba(x)[:,1]\n",
    "            eval_name1 = \"auc\"\n",
    "            eval_fn1 = lambda *args, **kwargs: roc_auc_score(*args, **kwargs)\n",
    "            eval_params1 = {}\n",
    "            eval_name2 = \"log_loss\"\n",
    "            eval_fn2 = log_loss\n",
    "        elif trial.task.problem == \"multiclass\":\n",
    "            predict_fn = lambda x: est.predict_proba(x)\n",
    "            eval_name1 = \"multi_auc\"\n",
    "            eval_fn1 = lambda *args, **kwargs: roc_auc_score(*args, **kwargs)\n",
    "            eval_params1 = {\"average\": \"weighted\", \"multi_class\": \"ovr\"}\n",
    "            eval_name2 = \"cross_entropy\"\n",
    "            eval_fn2 = log_loss\n",
    "    elif trial.task.problem == \"regression\":\n",
    "        if trial.method.name == \"xgboost-base\":\n",
    "            est = XGBRegressor(enable_categorical=True)\n",
    "        elif trial.method.name == \"ebm-base\":\n",
    "            est = ExplainableBoostingRegressor(**extra_params)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Method unavailable for {trial.method.name}\")\n",
    "            \n",
    "        q75, q25 = np.percentile(y_train, [75, 25])\n",
    "        interquartile_range = q75 - q25\n",
    "    \n",
    "        predict_fn = lambda x: est.predict(x)\n",
    "        eval_name1 = \"rmsdiqr\"\n",
    "        eval_fn1 = lambda *args, **kwargs: root_mean_squared_error(*args, **kwargs) / interquartile_range\n",
    "        eval_params1 = {}\n",
    "\n",
    "    global global_counter\n",
    "    try:\n",
    "        global_counter += 1\n",
    "    except NameError:\n",
    "        global_counter = 1\n",
    "    \n",
    "    # Train\n",
    "    start_time = time()\n",
    "    print(f\"FIT: {global_counter}, {trial.task.origin}, {trial.task.name}, {trial.method.name}, \", end=\"\")\n",
    "    with warnings.catch_warnings():  \n",
    "        warnings.filterwarnings(\"ignore\") \n",
    "        est.fit(X_train, y_train)\n",
    "    end_time = time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    trial.log(\"fit_time\", elapsed_time)\n",
    "    \n",
    "    # Predict\n",
    "    start_time = time()\n",
    "    predictions = predict_fn(X_test)\n",
    "    end_time = time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    trial.log(\"predict_time\", elapsed_time)\n",
    "\n",
    "    # Score\n",
    "    eval_score = eval_fn1(y_test, predictions, **eval_params1)\n",
    "    trial.log(eval_name1, eval_score)\n",
    "\n",
    "    if eval_fn2 is not None:\n",
    "        eval_score2 = eval_fn2(y_test, predictions)\n",
    "        trial.log(eval_name2, eval_score2)\n",
    "    \n",
    "    print(eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c226e77-753a-4e1c-bb6b-d80156845785",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_recreate=False\n",
    "exist_ok=True\n",
    "\n",
    "import uuid\n",
    "experiment_name = \"myexperiment\" + \"__\" + str(uuid.uuid4())\n",
    "print(\"Experiment name: \" + str(experiment_name))\n",
    "\n",
    "from powerlift.bench import retrieve_openml, retrieve_pmlb, retrieve_catboost_50k\n",
    "from powerlift.bench import Benchmark, Store, populate_with_datasets\n",
    "from powerlift.executors import LocalMachine\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "# Initialize database (if needed).\n",
    "store = Store(f\"sqlite:///{os.getcwd()}/powerlift.db\", force_recreate=force_recreate)\n",
    "\n",
    "cache_dir=\"~/.powerlift\"\n",
    "data_retrieval = chain(\n",
    "    # retrieve_catboost_50k(cache_dir=cache_dir),\n",
    "    # retrieve_pmlb(cache_dir=cache_dir),\n",
    "    retrieve_openml(cache_dir=cache_dir),\n",
    ")\n",
    "\n",
    "# This downloads datasets once and feeds into the database.\n",
    "populate_with_datasets(store, data_retrieval, exist_ok=exist_ok)\n",
    "\n",
    "# Run experiment\n",
    "benchmark = Benchmark(store, name=experiment_name)\n",
    "benchmark.run(trial_runner, trial_filter, executor=LocalMachine(store, debug_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79655404-4f7b-474a-aa6c-d74cb831980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.wait_until_complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70b5dc-d0ef-4b4a-93be-1fd8a60e2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-establish connection\n",
    "#benchmark = Benchmark(conn_str, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b43b3-8d9f-4cfd-81bf-4d4200deb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df = benchmark.status()\n",
    "for errmsg in status_df[\"errmsg\"]:\n",
    "    if errmsg is not None:\n",
    "        print(\"ERROR: \" + str(errmsg))\n",
    "print(status_df['status'].value_counts().to_string(index=True, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e071c6-655b-4800-852c-1b05a982156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = benchmark.results()\n",
    "results_df.to_csv('results.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5745059-f747-4c02-8720-0b2d49aa4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload if analyzing later\n",
    "import pandas as pd\n",
    "results_df = pd.read_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2716b066-7d8b-4163-8d5d-a1b7f0274dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "averages = results_df.groupby(['method', 'name'])['num_val'].mean().unstack().reset_index()\n",
    "\n",
    "metric_ranks = results_df.pivot_table('num_val', ['task', 'name'], 'method')\n",
    "metric_ranks = metric_ranks.rank(axis=1, ascending=True, method='min')\n",
    "metric_ranks = metric_ranks.groupby('name').mean().transpose()\n",
    "metric_ranks.columns = [f\"{col}_RANK\" for col in metric_ranks.columns]\n",
    "metric_ranks = metric_ranks.reset_index()\n",
    "\n",
    "overall_rank = results_df[results_df['name'].isin(['log_loss', 'cross_entropy', 'rmsdiqr'])]\n",
    "overall_rank = overall_rank.pivot_table('num_val', 'task', 'method')\n",
    "overall_rank = overall_rank.rank(axis=1, ascending=True, method='min')\n",
    "overall_rank = overall_rank.mean()\n",
    "overall_rank = overall_rank.to_frame(name='RANK').reset_index()\n",
    "\n",
    "desired_columns = ['method', 'RANK', 'auc', 'multi_auc', 'rmsdiqr', 'log_loss_RANK', 'cross_entropy_RANK', 'rmsdiqr_RANK', 'fit_time', 'predict_time']\n",
    "combined_df = averages.merge(metric_ranks, on='method').merge(overall_rank, on='method')\n",
    "combined_df = combined_df.reindex(columns=desired_columns)\n",
    "combined_df = combined_df.sort_values(by='RANK')\n",
    "\n",
    "print(combined_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
